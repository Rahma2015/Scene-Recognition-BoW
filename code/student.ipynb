{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.youtube.com/watch?v=jU5j7Jd6hy0\n",
    "#https://www.youtube.com/watch?v=IZAKJMgUmWc\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib\n",
    "from skimage.io import imread\n",
    "from skimage.color import rgb2grey\n",
    "from skimage.feature import hog\n",
    "from skimage.transform import resize\n",
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "\n",
    "def get_tiny_images(image_paths):\n",
    "    \"\"\"\n",
    "    This feature is inspired by the simple tiny images used as features in\n",
    "    80 million tiny images: a large dataset for non-parametric object and\n",
    "    scene recognition. A. Torralba, R. Fergus, W. T. Freeman. IEEE\n",
    "    Transactions on Pattern Analysis and Machine Intelligence, vol.30(11),\n",
    "    pp. 1958-1970, 2008. http://groups.csail.mit.edu/vision/TinyImages/\n",
    "\n",
    "    Inputs:\n",
    "        image_paths: a 1-D Python list of strings. Each string is a complete\n",
    "                     path to an image on the filesystem.\n",
    "    Outputs:\n",
    "        An n x d numpy array where n is the number of images and d is the\n",
    "        length of the tiny image representation vector. e.g. if the images\n",
    "        are resized to 16x16, then d is 16 * 16 = 256.\n",
    "\n",
    "    To build a tiny image feature, resize the original image to a very small\n",
    "    square resolution (e.g. 16x16). You can either resize the images to square\n",
    "    while ignoring their aspect ratio, or you can crop the images into squares\n",
    "    first and then resize evenly. Normalizing these tiny images will increase\n",
    "    performance modestly.\n",
    "\n",
    "    As you may recall from class, naively downsizing an image can cause\n",
    "    aliasing artifacts that may throw off your comparisons. See the docs for\n",
    "    skimage.transform.resize for details:\n",
    "    http://scikit-image.org/docs/dev/api/skimage.transform.html#skimage.transform.resize\n",
    "\n",
    "    Suggested functions: skimage.transform.resize, skimage.color.rgb2grey,\n",
    "                         skimage.io.imread, np.reshape\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "\n",
    "    return np.array([])\n",
    "\n",
    "\n",
    "def build_vocabulary(image_paths, vocab_size):\n",
    "    \"\"\"\n",
    "    This function should sample HOG descriptors from the training images,\n",
    "    cluster them with kmeans, and then return the cluster centers.\n",
    "\n",
    "    Inputs:\n",
    "        image_paths: a Python list of image path strings\n",
    "         vocab_size: an integer indicating the number of words desired for the\n",
    "                     bag of words vocab set\n",
    "\n",
    "    Outputs:\n",
    "        a vocab_size x (z*z*9) (see below) array which contains the cluster\n",
    "        centers that result from the K Means clustering.\n",
    "\n",
    "    You'll need to generate HOG features using the skimage.feature.hog() function.\n",
    "    The documentation is available here:\n",
    "    http://scikit-image.org/docs/dev/api/skimage.feature.html#skimage.feature.hog\n",
    "\n",
    "    However, the documentation is a bit confusing, so we will highlight some\n",
    "    important arguments to consider:\n",
    "        cells_per_block: The hog function breaks the image into evenly-sized\n",
    "            blocks, which are further broken down into cells, each made of\n",
    "            pixels_per_cell pixels (see below). Setting this parameter tells the\n",
    "            function how many cells to include in each block. This is a tuple of\n",
    "            width and height. Your SIFT implementation, which had a total of\n",
    "            16 cells, was equivalent to setting this argument to (4,4).\n",
    "        pixels_per_cell: This controls the width and height of each cell\n",
    "            (in pixels). Like cells_per_block, it is a tuple. In your SIFT\n",
    "            implementation, each cell was 4 pixels by 4 pixels, so (4,4).\n",
    "        feature_vector: This argument is a boolean which tells the function\n",
    "            what shape it should use for the return array. When set to True,\n",
    "            it returns one long array. We recommend setting it to True and\n",
    "            reshaping the result rather than working with the default value,\n",
    "            as it is very confusing.\n",
    "\n",
    "    It is up to you to choose your cells per block and pixels per cell. Choose\n",
    "    values that generate reasonably-sized feature vectors and produce good\n",
    "    classification results. For each cell, HOG produces a histogram (feature\n",
    "    vector) of length 9. We want one feature vector per block. To do this we\n",
    "    can append the histograms for each cell together. Let's say you set\n",
    "    cells_per_block = (z,z). This means that the length of your feature vector\n",
    "    for the block will be z*z*9.\n",
    "\n",
    "    With feature_vector=True, hog() will return one long np array containing every\n",
    "    cell histogram concatenated end to end. We want to break this up into a\n",
    "    list of (z*z*9) block feature vectors. We can do this using a really nifty numpy\n",
    "    function. When using np.reshape, you can set the length of one dimension to\n",
    "    -1, which tells numpy to make this dimension as big as it needs to be to\n",
    "    accomodate to reshape all of the data based on the other dimensions. So if\n",
    "    we want to break our long np array (long_boi) into rows of z*z*9 feature\n",
    "    vectors we can use small_bois = long_boi.reshape(-1, z*z*9).\n",
    "\n",
    "    The number of feature vectors that come from this reshape is dependent on\n",
    "    the size of the image you give to hog(). It will fit as many blocks as it\n",
    "    can on the image. You can choose to resize (or crop) each image to a consistent size\n",
    "    (therefore creating the same number of feature vectors per image), or you\n",
    "    can find feature vectors in the original sized image.\n",
    "\n",
    "    ONE MORE THING\n",
    "    If we returned all the features we found as our vocabulary, we would have an\n",
    "    absolutely massive vocabulary. That would make matching inefficient AND\n",
    "    inaccurate! So we use K Means clustering to find a much smaller (vocab_size)\n",
    "    number of representative points. We recommend using sklearn.cluster.KMeans\n",
    "    to do this. Note that this can take a VERY LONG TIME to complete (upwards\n",
    "    of ten minutes for large numbers of features and large max_iter), so set\n",
    "    the max_iter argument to something low (we used 100) and be patient. You\n",
    "    may also find success setting the \"tol\" argument (see documentation for\n",
    "    details)\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "\n",
    "    return np.array([])\n",
    "\n",
    "\n",
    "def get_bags_of_words(image_paths):\n",
    "    \"\"\"\n",
    "    This function should take in a list of image paths and calculate a bag of\n",
    "    words histogram for each image, then return those histograms in an array.\n",
    "\n",
    "    Inputs:\n",
    "        image_paths: A Python list of strings, where each string is a complete\n",
    "                     path to one image on the disk.\n",
    "\n",
    "    Outputs:\n",
    "        An nxd numpy matrix, where n is the number of images in image_paths and\n",
    "        d is size of the histogram built for each image.\n",
    "\n",
    "    Use the same hog function to extract feature vectors as before (see\n",
    "    build_vocabulary). It is important that you use the same hog settings for\n",
    "    both build_vocabulary and get_bags_of_words! Otherwise, you will end up\n",
    "    with different feature representations between your vocab and your test\n",
    "    images, and you won't be able to match anything at all!\n",
    "\n",
    "    After getting the feature vectors for an image, you will build up a\n",
    "    histogram that represents what words are contained within the image.\n",
    "    For each feature, find the closest vocab word, then add 1 to the histogram\n",
    "    at the index of that word. For example, if the closest vector in the vocab\n",
    "    is the 103rd word, then you should add 1 to the 103rd histogram bin. Your\n",
    "    histogram should have as many bins as there are vocabulary words.\n",
    "\n",
    "    Suggested functions: scipy.spatial.distance.cdist, np.argsort,\n",
    "                         np.linalg.norm, skimage.feature.hog\n",
    "    \"\"\"\n",
    "\n",
    "    vocab = np.load('vocab.npy')\n",
    "    print('Loaded vocab from file.')\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "\n",
    "    return np.array([])\n",
    "\n",
    "\n",
    "def svm_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    \"\"\"\n",
    "    This function will predict a category for every test image by training\n",
    "    15 many-versus-one linear SVM classifiers on the training data, then\n",
    "    using those learned classifiers on the testing data.\n",
    "\n",
    "    Inputs:\n",
    "        train_image_feats: An nxd numpy array, where n is the number of training\n",
    "                           examples, and d is the image descriptor vector size.\n",
    "        train_labels: An nx1 Python list containing the corresponding ground\n",
    "                      truth labels for the training data.\n",
    "        test_image_feats: An mxd numpy array, where m is the number of test\n",
    "                          images and d is the image descriptor vector size.\n",
    "\n",
    "    Outputs:\n",
    "        An mx1 numpy array of strings, where each string is the predicted label\n",
    "        for the corresponding image in test_image_feats\n",
    "\n",
    "    We suggest you look at the sklearn.svm module, including the LinearSVC\n",
    "    class. With the right arguments, you can get a 15-class SVM as described\n",
    "    above in just one call! Be sure to read the documentation carefully.\n",
    "    \"\"\"\n",
    "\n",
    "    # TODO: Implement this function!\n",
    "\n",
    "    return np.array([])\n",
    "\n",
    "\n",
    "def nearest_neighbor_classify(train_image_feats, train_labels, test_image_feats):\n",
    "    \"\"\"\n",
    "    This function will predict the category for every test image by finding\n",
    "    the training image with most similar features. You will complete the given\n",
    "    partial implementation of k-nearest-neighbors such that for any arbitrary\n",
    "    k, your algorithm finds the closest k neighbors and then votes among them\n",
    "    to find the most common category and returns that as its prediction.\n",
    "\n",
    "    Inputs:\n",
    "        train_image_feats: An nxd numpy array, where n is the number of training\n",
    "                           examples, and d is the image descriptor vector size.\n",
    "        train_labels: An nx1 Python list containing the corresponding ground\n",
    "                      truth labels for the training data.\n",
    "        test_image_feats: An mxd numpy array, where m is the number of test\n",
    "                          images and d is the image descriptor vector size.\n",
    "\n",
    "    Outputs:\n",
    "        An mx1 numpy list of strings, where each string is the predicted label\n",
    "        for the corresponding image in test_image_feats\n",
    "\n",
    "    The simplest implementation of k-nearest-neighbors gives an even vote to\n",
    "    all k neighbors found - that is, each neighbor in category A counts as one\n",
    "    vote for category A, and the result returned is equivalent to finding the\n",
    "    mode of the categories of the k nearest neighbors. A more advanced version\n",
    "    uses weighted votes where closer matches matter more strongly than far ones.\n",
    "    This is not required, but may increase performance.\n",
    "\n",
    "    Be aware that increasing k does not always improve performance - even\n",
    "    values of k may require tie-breaking which could cause the classifier to\n",
    "    arbitrarily pick the wrong class in the case of an even split in votes.\n",
    "    Additionally, past a certain threshold the classifier is considering so\n",
    "    many neighbors that it may expand beyond the local area of logical matches\n",
    "    and get so many garbage votes from a different category that it mislabels\n",
    "    the data. Play around with a few values and see what changes.\n",
    "\n",
    "    Useful functions:\n",
    "        scipy.spatial.distance.cdist, np.argsort, scipy.stats.mode\n",
    "    \"\"\"\n",
    "\n",
    "    k = 1\n",
    "\n",
    "    # Gets the distance between each test image feature and each train image feature\n",
    "    # e.g., cdist\n",
    "    distances = cdist(test_image_feats, train_image_feats, 'euclidean')\n",
    "\n",
    "    # TODO:\n",
    "    # 1) Find the k closest features to each test image feature in euclidean space\n",
    "    # 2) Determine the labels of those k features\n",
    "    # 3) Pick the most common label from the k\n",
    "    # 4) Store that label in a list\n",
    "\n",
    "    return np.array([])\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
